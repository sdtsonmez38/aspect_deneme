# -*- coding: utf-8 -*-
"""aspect5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P3utgDrtGgSmgaxM1BZQWK0Vy-KceYxu
"""

import pandas as pd

# Veri setinin yüklenmesi
dataset_path = '/content/inputsV3.xlsx'  # Colab'da dosyayı yükledikten sonra bu yol değişebilir
data = pd.read_excel(dataset_path)

# Veri setinin ilk birkaç satırının görüntülenmesi
data.head()

import pandas as pd
import re
import spacy
!pip install nltk # install the nltk module
import nltk # import the nltk module
from nltk.corpus import stopwords

# Gerekli NLTK verilerini ve SpaCy modelini yükle
nltk.download('punkt')
nltk.download('stopwords')
sp = spacy.load("en_core_web_sm")

# Durak kelimelerin listesi
stop_words = set(stopwords.words('english'))
print(stop_words)

# URL'lerin ve emojilerin kaldırılması
def remove_url_and_handle_emoji(text):
    # URL kaldırma
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    # Emoji kaldırma veya değiştirme
    emoji_pattern = re.compile("[" u"\U0001F600-\U0001F64F" u"\U0001F300-\U0001F5FF" u"\U0001F680-\U0001F6FF" u"\U0001F1E0-\U0001F1FF" "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'[EMOJI]', text)
    return text

# Lematizasyon ve durak kelime kaldırma
def lemmatize_and_remove_stopwords(text):
    token = sp(text)
    lemmatized_tokens = [word.lemma_ for word in token if not word.is_stop and word.is_alpha]
    return " ".join(lemmatized_tokens)

# Ana ön işleme fonksiyonu
def preprocessing(text):
    text = remove_url_and_handle_emoji(text)  # URL ve emojilerin kaldırılması
    text = text.lower()  # Küçük harfe çevirme
    processed_text = lemmatize_and_remove_stopwords(text)  # Lematizasyon ve durak kelimelerin kaldırılması
    return processed_text

# Ön işleme işlemini veri setinin 'Body' sütununa uygulama
data['Cleaned_Body'] = data['Body'].apply(preprocessing)

# Temizlenmiş veriyi kontrol etme
data.head()

# prompt: veri çerçevesinde bulunan "Cleaned_Body" sütunundaki verileri bir kelime parçalayıcı (tokenizer) ile işleyip sonuçları "Cleaned_Body2" sütununa kaydet

from nltk.tokenize import word_tokenize

def tokenize_text(text):
  tokens = word_tokenize(text)
  return tokens


data['Cleaned_Body2'] = data['Cleaned_Body'].apply(tokenize_text)

data.head()

# prompt: veri çerçevesindeki Cleaned_Body2 sütunundaki değerleri bir listeye dönüştür ve bu listenin uzunluğunu (kaç öğe içerdiğini) hesapla

cleaned_body2_list = data['Cleaned_Body2'].tolist()
list_length = len(cleaned_body2_list)

print("Cleaned_Body2 sütunundaki öğe sayısı:", list_length)

# prompt: sentiment ve quality sınıf sayısınındaki veri miktarlarını göster

import pandas as pd

# Sentiment ve quality sınıflarının sayısını hesaplayın
sentiment_counts = data['Sentiment'].value_counts()
quality_counts = data['Quality'].value_counts()

print("Sentiment Sınıf Sayıları:")
print(sentiment_counts)
print("\nQuality Sınıf Sayıları:")
print(quality_counts)

#veri setini görüntüle
data.head()

data_words = data['Cleaned_Body2'].values.tolist()
len(data_words)

import gensim.corpora as corpora

# Create Dictionary
id2word = corpora.Dictionary(data_words)
# Create Corpus
texts = data_words
# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
# View
print(corpus[:1][0][:30])

from gensim.models import LdaMulticore
from gensim.models import LdaModel
from pprint import pprint

# number of topics
num_topics = 8
# Build LDA model
lda_model = LdaMulticore(corpus=corpus, id2word=id2word,
                     num_topics=num_topics, iterations=400)
# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

# prompt: lda ile çıkan aspectleri görüntüle

# LDA modelinden aspectleri (konuları) görüntüle
for idx, topic in lda_model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))

# FastText modelini yeniden eğit veya yükle
from gensim.models import FastText

# Cleaned_Body2 sütunundaki tokenize edilmiş kelime listelerini kullanacağız
data_words = data['Cleaned_Body2'].values.tolist()

# FastText modelini oluştur ve eğit
fasttext_model = FastText(data_words, vector_size=100, window=5, min_count=1, workers=4, sg=1)

# Eğitilen modeli kaydetmek isterseniz
fasttext_model.save("FastText-Model-For-ABSA.bin")

# Aspect listesi
aspects = ["Synchronization","Usability","Health","Functionality","Usage","Performance","Login", "Diet"]

# Benzerlik hesaplayan fonksiyon
def get_similarity(text, aspect):
    try:
        # Text listede olduğundan kelimeleri birleştiriyoruz
        text = " ".join(text)
        return fasttext_model.wv.n_similarity(text, aspect)
    except:
        return 0

# Tqdm ile ilerleme çubuğunu kullanmak için
from tqdm import tqdm
tqdm.pandas()

# Her bir aspect için benzerlik sütunları ekleyelim
for aspect in aspects:
    data[aspect] = data['Cleaned_Body2'].progress_map(lambda text: get_similarity(text, aspect))

# Sonuçları kontrol edelim
data.head()

import torch
from torch import nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch.utils.data import RandomSampler
import warnings

class config:
    warnings.filterwarnings("ignore", category = UserWarning)
    IMG_SIZE = (224,224)
    DEVICE = ("cuda" if torch.cuda.is_available() else "cpu")
    FOLDS = 5
    SHUFFLE = True
    BATCH_SIZE = 32
    LR = 0.01
    EPOCHS = 30
    EMB_DIM = 100
    MAX_LEN = 20
    MODEL_PATH = "./Models/MyModel.pt"

data.head()

# prompt: # Veri setini CSV olarak kaydetme

# Veri setini CSV olarak kaydetme
data.to_csv('processed_data.csv', index=False)

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import numpy as np
from torch.nn.utils.rnn import pad_sequence

# 1. Dataset Oluşturma
class CustomDataset(Dataset):
    def __init__(self, df, text_column, label_column):
        self.df = df
        self.texts = df[text_column]
        self.labels = df[label_column].apply(lambda x: 1 if x == 'positive' else 0)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        text = self.texts.iloc[index]
        label = self.labels.iloc[index]

        # Check if text is a list and join it into a string if necessary
        if isinstance(text, list):
            text = ' '.join(text)

        # FastText ile metni embedding'e dönüştürme
        text_vector = [fasttext_model.wv[word] for word in text.split() if word in fasttext_model.wv]

        if len(text_vector) == 0:  # Eğer hiçbir kelime embedding'de yoksa, sıfır vektörü ekle
            text_vector = np.zeros((1, fasttext_model.vector_size))
        else:
            text_vector = np.array(text_vector)

        # LSTM'e uygun boyutta tensor haline getirme
        text_tensor = torch.tensor(text_vector, dtype=torch.float32)

        return text_tensor, torch.tensor(label, dtype=torch.float32)

# Eğitim ve doğrulama veri setini ayırma
train_df, val_df = train_test_split(data, test_size=0.2, random_state=42)

train_dataset = CustomDataset(train_df, 'Cleaned_Body2', 'Sentiment')
val_dataset = CustomDataset(val_df, 'Cleaned_Body2', 'Sentiment')

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: custom_collate_fn(x))
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=lambda x: custom_collate_fn(x))

# Padding fonksiyonu
def custom_collate_fn(batch):
    texts, labels = zip(*batch)
    texts_padded = pad_sequence(texts, padding_value=0.0)  # Batch_first kullanımı olmadan
    labels_tensor = torch.tensor(labels, dtype=torch.float32)
    return texts_padded, labels_tensor

# 2. Model Tanımlama

class SentimentModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(SentimentModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=False, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bidirectional olduğu için 2 kat hidden_dim
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        lstm_out, (hidden, _) = self.lstm(x)
        # LSTM çıkışını tamamen bağlantılı katmana geçir
        # hidden.view(-1, self.hidden_dim * 2) ile hidden layer'ın boyutunu ayarlıyoruz
        out = self.fc(hidden.view(-1, self.lstm.hidden_size * 2))  # Son hidden durumunu kullanıyoruz ve boyutunu ayarlıyoruz
        return self.sigmoid(out) # Output shape is now consistent with the labels

input_dim = 100  # FastText vektör boyutu
hidden_dim = 128
output_dim = 1

model = SentimentModel(input_dim, hidden_dim, output_dim)

# 3. Modelin Eğitimi
criterion = nn.BCELoss()  # Binary Cross Entropy Loss, pozitif/negatif sınıflandırma için uygun
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# GPU desteği için kontrol
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Eğitim döngüsü
for epoch in range(10):
    model.train()
    running_loss = 0.0
    for texts, labels in train_loader:
        texts, labels = texts.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(texts)  # LSTM'e giriş olarak uygun formatta olmalı
        loss = criterion(outputs, labels.unsqueeze(1))
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)
    print(f'Epoch [{epoch+1}/10], Loss: {avg_loss:.4f}')

# Modelin doğrulanması
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for texts, labels in val_loader:
        texts, labels = texts.to(device), labels.to(device)
        outputs = model(texts)
        predicted = (outputs > 0.5).float()
        total += labels.size(0)
        correct += (predicted.view(-1) == labels).sum().item()

accuracy = 100 * correct / total
print(f'Validation Accuracy: {accuracy:.2f}%')

# Modelin kaydedilmesi
torch.save(model.state_dict(), 'sentiment_model.pth')

import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel, BertForSequenceClassification
from transformers import AdamW
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset

# Configurations
class Config:
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    BATCH_SIZE = 16
    EPOCHS = 4
    LR = 2e-5
    MAX_LEN = 128  # BERT için maksimum giriş uzunluğu
    MODEL_NAME = 'bert-base-uncased'  # BERT model ismi

config = Config()

# Veri setini yükleyin
data = pd.read_csv('/content/processed_data.csv')

# BERT Tokenizer'ı yükleyin
tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)

# Tokenize etme ve BERT için veri hazırlama
def encode_data(texts, labels, max_len):
    input_ids = []
    attention_masks = []

    for text in texts:
        encoded = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=max_len,
            pad_to_max_length=True,
            return_attention_mask=True,
            truncation=True
        )
        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])

    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels)

# Veri setini bölme (train/test ayırma)
train_texts, val_texts, train_labels, val_labels = train_test_split(data['Cleaned_Body'], data['Sentiment'], test_size=0.2)

# Etiketleri sayısal değerlere dönüştürün (label encoding)
train_labels = train_labels.apply(lambda x: 1 if x == 'positive' else -1 if x=='negative' else 0)
val_labels = val_labels.apply(lambda x: 1 if x == 'positive' else -1  if x=='negative' else 0)

# Veriyi encode etme
train_inputs, train_masks, train_labels = encode_data(train_texts.tolist(), train_labels.tolist(), config.MAX_LEN)
val_inputs, val_masks, val_labels = encode_data(val_texts.tolist(), val_labels.tolist(), config.MAX_LEN)

# TensorDataset ve DataLoader oluşturma
train_dataset = TensorDataset(train_inputs, train_masks, train_labels)
val_dataset = TensorDataset(val_inputs, val_masks, val_labels)

train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE)

# BERT tabanlı sınıflandırma modelini yükleyin
model = BertForSequenceClassification.from_pretrained(config.MODEL_NAME, num_labels=2)
model.to(config.DEVICE)

# Optimizer ve Loss Fonksiyonu
optimizer = AdamW(model.parameters(), lr=config.LR)
criterion = nn.CrossEntropyLoss()

# Model eğitimi
for epoch in range(config.EPOCHS):
    model.train()
    running_loss = 0.0
    correct_preds = 0
    total_preds = 0

    for batch in train_loader:
        input_ids, attention_masks, labels = [x.to(config.DEVICE) for x in batch]

        optimizer.zero_grad()

        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)
        loss = outputs.loss
        logits = outputs.logits

        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, preds = torch.max(logits, dim=1)
        correct_preds += torch.sum(preds == labels).item()
        total_preds += labels.size(0)

    avg_loss = running_loss / len(train_loader)
    accuracy = correct_preds / total_preds * 100
    print(f'Epoch [{epoch+1}/{config.EPOCHS}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')

# Modelin doğrulanması (Validation)
model.eval()
correct_preds = 0
total_preds = 0

with torch.no_grad():
    for batch in val_loader:
        input_ids, attention_masks, labels = [x.to(config.DEVICE) for x in batch]

        outputs = model(input_ids, attention_mask=attention_masks)
        logits = outputs.logits
        _, preds = torch.max(logits, dim=1)
        correct_preds += torch.sum(preds == labels).item()
        total_preds += labels.size(0)

val_accuracy = correct_preds / total_preds * 100
print(f'Validation Accuracy: {val_accuracy:.2f}%')

# Modeli kaydet
model.save_pretrained('./bert-sentiment-model')
tokenizer.save_pretrained('./bert-sentiment-tokenizer')

# prompt: # Yorum için en iyi aspect'i bulmak

def find_best_aspect(row):
  """
  Bir satır için en yüksek benzerliğe sahip aspect'i bulur.

  Args:
    row: DataFrame'deki bir satır.

  Returns:
    En yüksek benzerliğe sahip aspect'in adı.
  """
  aspect_columns = ['Synchronization', 'Usability', 'Health', 'Functionality', 'Usage', 'Performance', 'Login', 'Diet']
  max_similarity = 0
  best_aspect = None

  for aspect in aspect_columns:
    if row[aspect] > max_similarity:
      max_similarity = row[aspect]
      best_aspect = aspect

  return best_aspect

# Her satır için en iyi aspect'i bul ve yeni bir sütuna ekle
data['Best_Aspect'] = data.apply(find_best_aspect, axis=1)

# Sonuçları kontrol et
data.head()

# prompt: en son oluşan veri setini göster

print(data.head())

# prompt: veri setini xlsx olarak kaydet

data.to_excel('processed_data.xlsx', index=False)

# prompt: en iyi aspect sayıları toplam satırı kontrol et

best_aspect_counts = data['Best_Aspect'].value_counts()
total_rows = len(data)

print("En İyi Aspect Sayıları:")
print(best_aspect_counts)
print("\nToplam Satır Sayısı:", total_rows)

# prompt: aspectlerin türkçelerini yaz sonuçları konrol et ve Body, Sentiment, Quality, Cleaned_Body2 ve 	Best_Aspect	 sütunlarından yeni dataframe oluştur

# Aspect'lerin Türkçe karşılıkları
aspect_translations = {
    "Synchronization": "Senkronizasyon",
    "Usability": "Kullanılabilirlik",
    "Health": "Sağlık",
    "Functionality": "İşlevsellik",
    "Usage": "Kullanım",
    "Performance": "Performans",
    "Login": "Giriş",
    "Diet": "Diyet"
}

# Best_Aspect sütunundaki değerleri Türkçe'ye çevirme
data['Best_Aspect_TR'] = data['Best_Aspect'].map(aspect_translations)

# Sonuçları kontrol etme
print(data.head())

# Yeni DataFrame oluşturma
new_df = data[['Body', 'Sentiment', 'Quality', 'Cleaned_Body2', 'Best_Aspect']]

# Yeni DataFrame'i görüntüle
new_df.head()

# prompt: data veri setinin Cleaned_Body alanını kullanarak Quality değerlerini bulan CNN derin öğrenme algoritması ile sınıflandır

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense
from tensorflow.keras.utils import to_categorical
!pip install scikit-learn

# Veri setini yükle
data = pd.read_excel('processed_data.xlsx')

# Quality sütunundaki kategorileri sayısal etiketlere dönüştür
# Quality sütunundaki benzersiz değerleri al
quality_categories = data['Quality'].unique()

# Kategorileri sayısal etiketlere eşleyen bir sözlük oluştur
category_to_label = {category: label for label, category in enumerate(quality_categories)}

# Quality sütunundaki değerleri sayısal etiketlere dönüştür
data['Quality_Label'] = data['Quality'].map(category_to_label)

# Veri setini eğitim ve test kümelerine ayır
# Quality_Label sütununu hedef değişken olarak kullan
X_train, X_test, y_train, y_test = train_test_split(data['Cleaned_Body'], data['Quality_Label'], test_size=0.2, random_state=42)

# Metinleri sayısal değerlere dönüştürmek için tokenizer oluştur
tokenizer = Tokenizer(num_words=5000) # En sık kullanılan 5000 kelimeyi dikkate al
tokenizer.fit_on_texts(X_train)

# Eğitim ve test verilerini tokenizer ile dönüştür
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Dönüştürülmüş verileri eşitlemek için padding uygula
max_length = 100 # Maksimum metin uzunluğu
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

# Etiketleri one-hot encoding ile dönüştür
y_train_cat = to_categorical(y_train)
y_test_cat = to_categorical(y_test)

# CNN modeli oluştur
model = Sequential()
model.add(Embedding(5000, 128, input_length=max_length))
model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(10, activation='relu'))
model.add(Dense(y_train_cat.shape[1], activation='softmax'))

# Modeli derle
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Modeli eğit
model.fit(X_train_pad, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test_cat))

# Modelin performansını değerlendir
loss, accuracy = model.evaluate(X_test_pad, y_test_cat)
print('Loss:', loss)
print('Accuracy:', accuracy)

# prompt: data veri setinin Cleaned_Body alanını kullanarak Quality değerlerini bulan BERT derin öğrenme algoritması ile sınıflandır

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import AdamW
import torch
from torch.utils.data import DataLoader, TensorDataset
from torch import nn

# Veri setini yükleyin
data = pd.read_excel('processed_data.xlsx')

# Quality sütunundaki kategorileri sayısal etiketlere dönüştür
# Quality sütunundaki benzersiz değerleri al
quality_categories = data['Quality'].unique()

# Kategorileri sayısal etiketlere eşleyen bir sözlük oluştur
category_to_label = {category: label for label, category in enumerate(quality_categories)}

# Quality sütunundaki değerleri sayısal etiketlere dönüştür
data['Quality_Label'] = data['Quality'].map(category_to_label)

# Veri setini eğitim ve test kümelerine ayır
# Quality_Label sütununu hedef değişken olarak kullan
X_train, X_test, y_train, y_test = train_test_split(data['Cleaned_Body'], data['Quality_Label'], test_size=0.2, random_state=42)


# Configurations
class Config:
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    BATCH_SIZE = 16
    EPOCHS = 4
    LR = 2e-5
    MAX_LEN = 128  # BERT için maksimum giriş uzunluğu
    MODEL_NAME = 'bert-base-uncased'  # BERT model ismi

config = Config()

# BERT Tokenizer'ı yükleyin
tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)


# Tokenize etme ve BERT için veri hazırlama
def encode_data(texts, labels, max_len):
    input_ids = []
    attention_masks = []

    for text in texts:
        encoded = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=max_len,
            pad_to_max_length=True,
            return_attention_mask=True,
            truncation=True
        )
        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])

    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels)

# Veriyi encode etme
train_inputs, train_masks, train_labels = encode_data(X_train.tolist(), y_train.tolist(), config.MAX_LEN)
test_inputs, test_masks, test_labels = encode_data(X_test.tolist(), y_test.tolist(), config.MAX_LEN)

# TensorDataset ve DataLoader oluşturma
train_dataset = TensorDataset(train_inputs, train_masks, train_labels)
test_dataset = TensorDataset(test_inputs, test_masks, test_labels)

train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE)

# BERT tabanlı sınıflandırma modelini yükleyin
model = BertForSequenceClassification.from_pretrained(config.MODEL_NAME, num_labels=len(quality_categories))
model.to(config.DEVICE)

# Optimizer ve Loss Fonksiyonu
optimizer = AdamW(model.parameters(), lr=config.LR)
criterion = nn.CrossEntropyLoss()

# Model eğitimi
for epoch in range(config.EPOCHS):
    model.train()
    running_loss = 0.0
    correct_preds = 0
    total_preds = 0

    for batch in train_loader:
        input_ids, attention_masks, labels = [x.to(config.DEVICE) for x in batch]

        optimizer.zero_grad()

        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)
        loss = outputs.loss
        logits = outputs.logits

        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, preds = torch.max(logits, dim=1)
        correct_preds += torch.sum(preds == labels).item()
        total_preds += labels.size(0)

    avg_loss = running_loss / len(train_loader)
    accuracy = correct_preds / total_preds * 100
    print(f'Epoch [{epoch+1}/{config.EPOCHS}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')

# Modelin doğrulanması (Validation)
model.eval()
correct_preds = 0
total_preds = 0

with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_masks, labels = [x.to(config.DEVICE) for x in batch]

        outputs = model(input_ids, attention_mask=attention_masks)
        logits = outputs.logits
        _, preds = torch.max(logits, dim=1)
        correct_preds += torch.sum(preds == labels).item()
        total_preds += labels.size(0)

test_accuracy = correct_preds / total_preds * 100
print(f'Test Accuracy: {test_accuracy:.2f}%')

# Modeli kaydet
model.save_pretrained('./bert-quality-model')
tokenizer.save_pretrained('./bert-quality-tokenizer')

# prompt: data veri setinin Cleaned_Body alanını kullanarak Quality değerlerini bulan 4 farklı derin öğrenme yöntemleri ile sınıflandır

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import numpy as np
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.layers import LSTM, Bidirectional
from tensorflow.keras.layers import GRU
from tensorflow.keras import layers

# Veri setini yükle
data = pd.read_excel('processed_data.xlsx')

# Quality sütunundaki kategorileri sayısal etiketlere dönüştür
label_encoder = LabelEncoder()
data['Quality_Label'] = label_encoder.fit_transform(data['Quality'])

# Veri setini eğitim ve test kümelerine ayır
X_train, X_test, y_train, y_test = train_test_split(
    data['Cleaned_Body2'], data['Quality_Label'], test_size=0.2, random_state=42
)

# Metinleri sayısal değerlere dönüştürmek için tokenizer oluştur
tokenizer = Tokenizer(num_words=5000)  # En sık kullanılan 5000 kelimeyi dikkate al
tokenizer.fit_on_texts(X_train)

# Eğitim ve test verilerini tokenizer ile dönüştür
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Dönüştürülmüş verileri eşitlemek için padding uygula
max_length = 100  # Maksimum metin uzunluğu
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

# Etiketleri one-hot encoding ile dönüştür
num_classes = len(np.unique(y_train))
y_train_cat = to_categorical(y_train, num_classes=num_classes)
y_test_cat = to_categorical(y_test, num_classes=num_classes)

# 1. CNN Modeli
cnn_model = Sequential()
cnn_model.add(Embedding(5000, 128, input_length=max_length))
cnn_model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))
cnn_model.add(MaxPooling1D(pool_size=2))
cnn_model.add(Flatten())
cnn_model.add(Dense(10, activation='relu'))
cnn_model.add(Dense(num_classes, activation='softmax'))

cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
cnn_model.fit(X_train_pad, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test_cat))

cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test_pad, y_test_cat)
print('CNN Model:')
print('Loss:', cnn_loss)
print('Accuracy:', cnn_accuracy)


# 2. RNN (LSTM) Modeli
rnn_model = Sequential()
rnn_model.add(Embedding(5000, 128, input_length=max_length))
rnn_model.add(LSTM(128))
rnn_model.add(Dense(num_classes, activation='softmax'))

rnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
rnn_model.fit(X_train_pad, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test_cat))

rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test_pad, y_test_cat)
print('\nRNN (LSTM) Model:')
print('Loss:', rnn_loss)
print('Accuracy:', rnn_accuracy)


# 3. RNN (GRU) Modeli
gru_model = Sequential()
gru_model.add(Embedding(5000, 128, input_length=max_length))
gru_model.add(GRU(128))
gru_model.add(Dense(num_classes, activation='softmax'))

gru_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
gru_model.fit(X_train_pad, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test_cat))

gru_loss, gru_accuracy = gru_model.evaluate(X_test_pad, y_test_cat)
print('\nRNN (GRU) Model:')
print('Loss:', gru_loss)
print('Accuracy:', gru_accuracy)


# 4. Bidirectional LSTM Modeli
bilstm_model = Sequential()
bilstm_model.add(Embedding(5000, 128, input_length=max_length))
bilstm_model.add(Bidirectional(LSTM(64)))
bilstm_model.add(Dense(num_classes, activation='softmax'))

bilstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
bilstm_model.fit(X_train_pad, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test_cat))

bilstm_loss, bilstm_accuracy = bilstm_model.evaluate(X_test_pad, y_test_cat)
print('\nBidirectional LSTM Model:')
print('Loss:', bilstm_loss)
print('Accuracy:', bilstm_accuracy)